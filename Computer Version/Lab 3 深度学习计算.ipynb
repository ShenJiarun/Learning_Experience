{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3820737d-2048-468a-8d59-78829b021bd8",
   "metadata": {},
   "source": [
    "# 3. 深度学习计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab630f-d4b2-422b-8311-7e46cbafd385",
   "metadata": {},
   "source": [
    "# 层和块\n",
    "\n",
    "之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。\n",
    "在这里，整个模型只有一个输出。\n",
    "注意，单个神经网络\n",
    "（1）接受一些输入；\n",
    "（2）生成相应的标量输出；\n",
    "（3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。\n",
    "\n",
    "然后，当考虑具有多个输出的网络时，\n",
    "我们利用矢量化算法来描述整层神经元。\n",
    "像单个神经元一样，层（1）接受一组输入，\n",
    "（2）生成相应的输出，\n",
    "（3）由一组可调整参数描述。\n",
    "当我们使用softmax回归时，一个单层本身就是模型。\n",
    "然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。\n",
    "\n",
    "对于多层感知机而言，整个模型及其组成层都是这种架构。\n",
    "整个模型接受原始输入（特征），生成输出（预测），\n",
    "并包含一些参数（所有组成层的参数集合）。\n",
    "同样，每个单独的层接收输入（由前一层提供），\n",
    "生成输出（到下一层的输入），并且具有一组可调参数，\n",
    "这些参数根据从下一层反向传播的信号进行更新。\n",
    "\n",
    "事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。\n",
    "例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，\n",
    "这些层是由*层组*（groups of layers）的重复模式组成。\n",
    "这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛的识别和检测任务。\n",
    "目前ResNet架构仍然是许多视觉任务的首选架构。\n",
    "在其他的领域，如自然语言处理和语音，\n",
    "层组以各种重复模式排列的类似架构现在也是普遍存在。\n",
    "\n",
    "为了实现这些复杂的网络，我们引入了神经网络*块*的概念。\n",
    "*块*（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n",
    "使用块进行抽象的一个好处是可以将一些块组合成更大的组件，\n",
    "这一过程通常是递归的。\n",
    "通过定义代码来按需生成任意复杂度的块，\n",
    "我们可以通过简洁的代码实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成块，形成更大的模型](http://d2l.ai/_images/blocks.svg)\n",
    "\n",
    "\n",
    "从编程的角度来看，块由*类*（class）表示。\n",
    "它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，\n",
    "并且必须存储任何必需的参数。\n",
    "注意，有些块不需要任何参数。\n",
    "最后，为了计算梯度，块必须具有反向传播函数。\n",
    "在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。\n",
    "\n",
    "在构造自定义块之前，(**我们先回顾一下多层感知机**)的代码。\n",
    "下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，\n",
    "然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fe87c0-c73b-45cb-b2cd-a99f59a39528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1719,  0.0631, -0.2184, -0.1255,  0.2625, -0.0102,  0.2537,  0.1314,\n",
       "         -0.0533, -0.0662],\n",
       "        [ 0.0338,  0.2080, -0.0491, -0.2104,  0.2280,  0.1018,  0.1739,  0.1283,\n",
       "         -0.1370, -0.1016]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 一个简单的两层感知机\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcc6f5-8fbf-49bd-8ec9-6d02d2a166e2",
   "metadata": {},
   "source": [
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，(**`nn.Sequential`定义了一种特殊的`Module`**)，\n",
    "即在PyTorch中表示一个块的类，\n",
    "它维护了一个由`Module`组成的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "`Linear`类本身就是`Module`的子类。\n",
    "另外，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.__call__(X)`的简写。\n",
    "这个前向传播函数非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214c2b6-34f9-40c8-885e-a7c7c901c213",
   "metadata": {},
   "source": [
    "## **自定义块**\n",
    "\n",
    "要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。\n",
    "在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。\n",
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "1. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问前向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。\n",
    "\n",
    "在下面的代码片段中，我们从零开始编写一个块。\n",
    "它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。\n",
    "注意，下面的`MLP`类继承了表示块的类。\n",
    "我们的实现只需要提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc8fc38-63e4-4c45-b572-4db0ae26a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4aa81-871a-4fee-a39a-12ddfa291c47",
   "metadata": {},
   "source": [
    "我们首先看一下前向传播函数，它以`X`作为输入，\n",
    "计算带有激活函数的隐藏表示，并输出其未规范化的输出值。\n",
    "在这个`MLP`实现中，两个层都是实例变量。\n",
    "要了解这为什么是合理的，可以想象实例化两个多层感知机（`net1`和`net2`），\n",
    "并根据不同的数据对它们进行训练。\n",
    "当然，我们希望它们学到两种不同的模型。\n",
    "\n",
    "接着我们[**实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层**]。\n",
    "注意一些关键细节：\n",
    "首先，我们定制的`__init__`函数通过`super().__init__()`\n",
    "调用父类的`__init__`函数，\n",
    "省去了重复编写模版代码的痛苦。\n",
    "然后，我们实例化两个全连接层，\n",
    "分别为`self.hidden`和`self.out`。\n",
    "注意，除非我们实现一个新的运算符，\n",
    "否则我们不必担心反向传播函数或参数初始化，\n",
    "系统将自动生成这些。\n",
    "\n",
    "我们来试一下这个函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7688dcc9-80b4-442f-9502-cc8264c7fc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2142, -0.1846,  0.2711,  0.0100,  0.0532,  0.2540,  0.3019,  0.0761,\n",
       "         -0.1798,  0.1380],\n",
       "        [-0.1012, -0.1309,  0.2606,  0.0119,  0.0270,  0.2167,  0.2997, -0.0449,\n",
       "         -0.1932,  0.2505]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b457fff-33b7-4e73-a98a-f2c0a5a61879",
   "metadata": {},
   "source": [
    "块的一个主要优点是它的多功能性。\n",
    "我们可以子类化块以创建层（如全连接层的类）、\n",
    "整个模型（如上面的`MLP`类）或具有中等复杂度的各种组件。\n",
    "我们在接下来的章节中充分利用了这种多功能性，\n",
    "比如在处理卷积神经网络时。\n",
    "\n",
    "## **顺序块**\n",
    "\n",
    "现在我们可以更仔细地看看`Sequential`类是如何工作的，\n",
    "回想一下`Sequential`的设计是为了把其他模块串起来。\n",
    "为了构建我们自己的简化的`MySequential`，\n",
    "我们只需要定义两个关键函数：\n",
    "\n",
    "1. 一种将块逐个追加到列表中的函数；\n",
    "1. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b392996e-8221-4385-bcdb-ee61dd752bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608aab4-11f4-444c-b1d2-c900109345cc",
   "metadata": {},
   "source": [
    "`__init__`函数将每个模块逐个添加到有序字典`_modules`中。\n",
    "读者可能会好奇为什么每个`Module`都有一个`_modules`属性？\n",
    "以及为什么我们使用它而不是自己定义一个Python列表？\n",
    "简而言之，`_modules`的主要优点是：\n",
    "在模块的参数初始化过程中，\n",
    "系统知道在`_modules`字典中查找需要初始化参数的子块。\n",
    "\n",
    "\n",
    "当`MySequential`的前向传播函数被调用时，\n",
    "每个添加的块都按照它们被添加的顺序执行。\n",
    "现在可以使用我们的`MySequential`类重新实现多层感知机。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601c5bb-bdb8-4467-a777-31e91113fdab",
   "metadata": {},
   "source": [
    "请注意，`MySequential`的用法与之前为`Sequential`类编写的代码相同。\n",
    "\n",
    "## **在前向传播函数中执行代码**\n",
    "\n",
    "`Sequential`类使模型构造变得简单，\n",
    "允许我们组合新的架构，而不必定义自己的类。\n",
    "然而，并不是所有的架构都是简单的顺序架构。\n",
    "当需要更强的灵活性时，我们需要定义自己的块。\n",
    "例如，我们可能希望在前向传播函数中执行Python的控制流。\n",
    "此外，我们可能希望执行任意的数学运算，\n",
    "而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "到目前为止，\n",
    "我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n",
    "然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，\n",
    "我们称之为*常数参数*（constant parameter）。\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。\n",
    "因此我们实现了一个`FixedHiddenMLP`类，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5a8d9f-c7a6-47f1-a36c-e31bdde6a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86faaa47-3274-4af8-8093-6e19332dd739",
   "metadata": {},
   "source": [
    "### Summary of Differences\n",
    "Input Requirements:\n",
    " - torch.mm() strictly requires both inputs to be 2D tensors.\n",
    " - torch.matmul() supports 1D, 2D, and higher-dimensional tensors, allowing for more flexible and varied matrix multiplication scenarios.\n",
    "\n",
    "Functionality:\n",
    " - torch.mm() is designed specifically for matrix multiplication of 2D tensors.\n",
    " - torch.matmul() is a more general function that can handle various types of tensor multiplications, including dot products, matrix-vector multiplications, matrix multiplications, and batched matrix multiplications.\n",
    "\n",
    "Practical Usage\n",
    " - Use torch.mm() when you are sure that both of your inputs are 2D matrices.\n",
    " - Use torch.matmul() for more general use cases, especially when dealing with tensors that are not strictly 2D or when performing batched operations.\n",
    "\n",
    "#### Understanding these differences will help you choose the appropriate function for your specific tensor operations and ensure that your code handles different types of tensor multiplications correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f480f-98e5-4dd3-8268-2f05779bc798",
   "metadata": {},
   "source": [
    "在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，\n",
    "其权重（`self.rand_weight`）在实例化时被随机初始化，之后为常量。\n",
    "这个权重不是一个模型参数，因此它永远不会被反向传播更新。\n",
    "然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情：\n",
    "它运行了一个while循环，在$L_1$范数大于$1$的条件下，\n",
    "将输出向量除以$2$，直到它满足条件为止。\n",
    "最后，模型返回了`X`中所有项的和。\n",
    "注意，此操作可能不会常用于在任何实际任务中，\n",
    "**我们只展示如何将任意代码集成到神经网络计算的流程中**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a755072-5b67-4697-b57d-1ae25dbb915d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2634, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648eeccf-1b89-442e-9814-7ce2fa376a74",
   "metadata": {},
   "source": [
    "**我们可以混合搭配各种组合块的方法。 在下面的例子中，我们以一些想到的方法嵌套块。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636f0c79-2358-4458-b85d-9583351db262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1070, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124e3a9-63d3-4707-aec1-015ab2eabaff",
   "metadata": {},
   "source": [
    "## 效率\n",
    "读者可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c44d2-da02-4481-80a0-d2a77351afca",
   "metadata": {},
   "source": [
    "1. 如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "\n",
    "2. 实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。\n",
    "\n",
    "3. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89f9c1c-2023-4181-8285-37de20617abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6793, 0.5776, 0.0299, 0.6901, 0.4119, 0.8059, 0.9413, 0.8777, 0.3409,\n",
       "         0.0738, 0.2525, 0.8563, 0.5847, 0.9011, 0.8983, 0.5051, 0.5203, 0.1404,\n",
       "         0.5656, 0.6879],\n",
       "        [0.4903, 0.4111, 0.8778, 0.4911, 0.3628, 0.8324, 0.8178, 0.0507, 0.1604,\n",
       "         0.5660, 0.4997, 0.1527, 0.5298, 0.4181, 0.0701, 0.2927, 0.4525, 0.8951,\n",
       "         0.6622, 0.1924]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question1\n",
    "\n",
    "class MySequential2(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules = []\n",
    "        for _, module in enumerate(args):\n",
    "            self.modules.append(module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self.modules:\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net2 = MySequential2()\n",
    "net2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c825e5-965d-494c-ad0f-4c98df93c6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6793, 0.5776, 0.0299, 0.6901, 0.4119, 0.8059, 0.9413, 0.8777, 0.3409,\n",
       "         0.0738, 0.2525, 0.8563, 0.5847, 0.9011, 0.8983, 0.5051, 0.5203, 0.1404,\n",
       "         0.5656, 0.6879],\n",
       "        [0.4903, 0.4111, 0.8778, 0.4911, 0.3628, 0.8324, 0.8178, 0.0507, 0.1604,\n",
       "         0.5660, 0.4997, 0.1527, 0.5298, 0.4181, 0.0701, 0.2927, 0.4525, 0.8951,\n",
       "         0.6622, 0.1924]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = MySequential()\n",
    "net1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f3e71-b763-483d-9cc9-c260896271d7",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d504d57-018a-47f9-ac41-66c374aacae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2431],\n",
       "        [0.2396]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4fdd1-c29f-4920-8dcd-a55b0b2bd947",
   "metadata": {},
   "source": [
    "## **参数访问**\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4f2f6c-29a9-4433-b31d-e68ace1e50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.4685,  0.1523,  0.4276,  0.2170],\n",
      "        [ 0.2070, -0.2701, -0.3235,  0.4819],\n",
      "        [-0.1636, -0.4048,  0.0479, -0.0216],\n",
      "        [-0.3541, -0.3916,  0.2523,  0.2355],\n",
      "        [ 0.0178, -0.3427, -0.2149, -0.2686],\n",
      "        [ 0.0372, -0.2371,  0.3755,  0.1828],\n",
      "        [-0.2516, -0.0220, -0.3187, -0.4787],\n",
      "        [ 0.3847,  0.0247,  0.3976, -0.3983]])), ('bias', tensor([ 0.1047, -0.3815, -0.2733, -0.3953, -0.4018, -0.3557,  0.4152, -0.3523]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f9cccd-c666-4800-88c2-75f241edbc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "print(net[1].state_dict()) ## ReLU层没有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df56da2-dc4e-4a95-b8bf-981309556f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.3475, -0.2010,  0.1273, -0.2447,  0.2884,  0.0401,  0.2663,  0.2480]])), ('bias', tensor([0.1481]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict()) \n",
    "## 参数和输入维度是转置关系，这样矩阵才能相乘。所以这里的矩阵是1*8而不是我们定义的8*1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aadfac-49d1-4586-b8bf-be83d78cd475",
   "metadata": {},
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### **目标参数**\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6735dd3-4158-4c77-9027-6d9c2523c7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1481], requires_grad=True)\n",
      "tensor([0.1481])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)  ## 我们可以使用.data来访问这个参数的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd0e089-7fe8-4548-b2a3-2b4c650ab53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3475, -0.2010,  0.1273, -0.2447,  0.2884,  0.0401,  0.2663,  0.2480]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 0.3475, -0.2010,  0.1273, -0.2447,  0.2884,  0.0401,  0.2663,  0.2480]])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].weight)\n",
    "print(net[2].weight.data)\n",
    "print(net[2].weight.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6158a-199d-4208-8c1c-e43c5c974c8c",
   "metadata": {},
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31cb80cd-9010-474a-ad9b-15830960257b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b77d6-927b-4904-8e6b-482a1df9919e",
   "metadata": {},
   "source": [
    "### **一次性访问所有参数**\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94bc95ea-4382-4eea-bc1c-daaa538fcebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dbf39b7-cb2f-4798-969f-bd28092fe677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[-0.4685,  0.1523,  0.4276,  0.2170],\n",
      "        [ 0.2070, -0.2701, -0.3235,  0.4819],\n",
      "        [-0.1636, -0.4048,  0.0479, -0.0216],\n",
      "        [-0.3541, -0.3916,  0.2523,  0.2355],\n",
      "        [ 0.0178, -0.3427, -0.2149, -0.2686],\n",
      "        [ 0.0372, -0.2371,  0.3755,  0.1828],\n",
      "        [-0.2516, -0.0220, -0.3187, -0.4787],\n",
      "        [ 0.3847,  0.0247,  0.3976, -0.3983]], requires_grad=True)) ('bias', Parameter containing:\n",
      "tensor([ 0.1047, -0.3815, -0.2733, -0.3953, -0.4018, -0.3557,  0.4152, -0.3523],\n",
      "       requires_grad=True))\n",
      "('0.weight', Parameter containing:\n",
      "tensor([[-0.4685,  0.1523,  0.4276,  0.2170],\n",
      "        [ 0.2070, -0.2701, -0.3235,  0.4819],\n",
      "        [-0.1636, -0.4048,  0.0479, -0.0216],\n",
      "        [-0.3541, -0.3916,  0.2523,  0.2355],\n",
      "        [ 0.0178, -0.3427, -0.2149, -0.2686],\n",
      "        [ 0.0372, -0.2371,  0.3755,  0.1828],\n",
      "        [-0.2516, -0.0220, -0.3187, -0.4787],\n",
      "        [ 0.3847,  0.0247,  0.3976, -0.3983]], requires_grad=True)) ('0.bias', Parameter containing:\n",
      "tensor([ 0.1047, -0.3815, -0.2733, -0.3953, -0.4018, -0.3557,  0.4152, -0.3523],\n",
      "       requires_grad=True)) ('2.weight', Parameter containing:\n",
      "tensor([[ 0.3475, -0.2010,  0.1273, -0.2447,  0.2884,  0.0401,  0.2663,  0.2480]],\n",
      "       requires_grad=True)) ('2.bias', Parameter containing:\n",
      "tensor([0.1481], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a5b54-6c41-43ea-aa5e-29f40a0e8c07",
   "metadata": {},
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef805159-b481-46d7-9608-83ef52841b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1047, -0.3815, -0.2733, -0.3953, -0.4018, -0.3557,  0.4152, -0.3523])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a791e5-c5ee-47a0-b093-d09f04196745",
   "metadata": {},
   "source": [
    "### **从嵌套块收集参数**\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18eb6fd6-4141-48d3-bb20-c007ddb52109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1914],\n",
       "        [0.1914]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df2b8da-bb09-4026-b88b-b03d41dbbf84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.block 0.0.weight', tensor([[ 0.4534, -0.3344,  0.0375, -0.2485],\n",
      "        [ 0.2532,  0.0048,  0.2724, -0.1310],\n",
      "        [ 0.0075, -0.0177,  0.2449, -0.1703],\n",
      "        [-0.4178, -0.1385,  0.4274, -0.4289],\n",
      "        [-0.1031, -0.4183,  0.3720,  0.2747],\n",
      "        [ 0.1418,  0.4607, -0.2048,  0.4437],\n",
      "        [-0.2948, -0.2325,  0.1778,  0.1342],\n",
      "        [-0.1966, -0.3470,  0.4549,  0.0425]])), ('0.block 0.0.bias', tensor([-0.1345,  0.3144, -0.2128,  0.3485,  0.0447,  0.2750, -0.1725, -0.4577])), ('0.block 0.2.weight', tensor([[-0.2414,  0.1234, -0.3389, -0.0876, -0.0691,  0.3348, -0.3191, -0.2875],\n",
      "        [ 0.1691, -0.0538,  0.0482, -0.3294, -0.0346, -0.0166, -0.1113,  0.3231],\n",
      "        [-0.1308,  0.0048,  0.0403,  0.1629,  0.2921,  0.2143, -0.2688, -0.0754],\n",
      "        [ 0.0236, -0.1183, -0.0849, -0.2337, -0.1574, -0.3059, -0.0702,  0.3110]])), ('0.block 0.2.bias', tensor([ 0.0278, -0.0063, -0.2406, -0.2715])), ('0.block 1.0.weight', tensor([[-0.3716,  0.0426,  0.4760,  0.1416],\n",
      "        [-0.1518,  0.0314,  0.3287,  0.2421],\n",
      "        [ 0.4955, -0.3581,  0.4539, -0.0836],\n",
      "        [-0.1234,  0.3504, -0.3047, -0.4848],\n",
      "        [-0.0232,  0.2442, -0.1397, -0.4270],\n",
      "        [-0.3667, -0.2639,  0.0368, -0.3581],\n",
      "        [ 0.1635, -0.2275, -0.2866,  0.4745],\n",
      "        [ 0.1138,  0.1289, -0.0993,  0.1094]])), ('0.block 1.0.bias', tensor([ 0.0311,  0.0300, -0.1807,  0.4228, -0.1311, -0.3480, -0.2724,  0.4388])), ('0.block 1.2.weight', tensor([[ 0.1799,  0.2817,  0.1826, -0.2483,  0.2277, -0.2317,  0.0343,  0.3402],\n",
      "        [-0.0447,  0.3260,  0.0039,  0.2002,  0.1368, -0.0641,  0.1508, -0.2149],\n",
      "        [ 0.3505,  0.1646, -0.0885, -0.0476, -0.2303,  0.0993,  0.2226, -0.1760],\n",
      "        [-0.3231,  0.1635,  0.0019,  0.1953,  0.2086,  0.2356, -0.3177,  0.1484]])), ('0.block 1.2.bias', tensor([-0.0180, -0.1628,  0.2513,  0.2828])), ('0.block 2.0.weight', tensor([[ 0.0888,  0.0678,  0.1260,  0.0012],\n",
      "        [ 0.2170,  0.1457, -0.4557,  0.0574],\n",
      "        [ 0.0747,  0.4039, -0.3310,  0.2928],\n",
      "        [ 0.1100, -0.4490,  0.1646, -0.0810],\n",
      "        [-0.2337,  0.0686, -0.3253, -0.4606],\n",
      "        [-0.2339,  0.3127,  0.1215,  0.0175],\n",
      "        [ 0.1567,  0.4961, -0.3290, -0.4524],\n",
      "        [-0.1040,  0.1698, -0.2341, -0.2377]])), ('0.block 2.0.bias', tensor([-0.3769,  0.2270,  0.4217, -0.0508,  0.0461,  0.0764, -0.0812,  0.2530])), ('0.block 2.2.weight', tensor([[-0.1610, -0.1948, -0.1043, -0.1215, -0.0282,  0.2583, -0.0252,  0.0601],\n",
      "        [-0.0071, -0.0540,  0.0233, -0.2840,  0.2067, -0.1861, -0.2885, -0.1971],\n",
      "        [-0.1084,  0.2378, -0.2678,  0.0487,  0.2908,  0.2647, -0.1901, -0.3354],\n",
      "        [-0.3097,  0.3002, -0.2387, -0.2053,  0.0755,  0.1713, -0.2548, -0.1546]])), ('0.block 2.2.bias', tensor([-0.0234,  0.1600, -0.2144, -0.2271])), ('0.block 3.0.weight', tensor([[-0.1516,  0.4565, -0.2368, -0.3129],\n",
      "        [-0.2442,  0.0720, -0.3771,  0.4180],\n",
      "        [ 0.2830, -0.2365,  0.3163, -0.2481],\n",
      "        [ 0.2589, -0.2721,  0.4958, -0.4568],\n",
      "        [-0.1473, -0.1402,  0.2431,  0.2737],\n",
      "        [-0.1630, -0.1923,  0.2162,  0.1286],\n",
      "        [ 0.3073,  0.4532,  0.3303,  0.0360],\n",
      "        [ 0.0787, -0.4098, -0.3411, -0.0096]])), ('0.block 3.0.bias', tensor([-0.1590, -0.2722, -0.3760,  0.0403, -0.0059,  0.1430,  0.0127,  0.4951])), ('0.block 3.2.weight', tensor([[ 3.1520e-01, -1.8698e-01,  1.8367e-01, -3.5110e-01, -7.9345e-02,\n",
      "          1.9692e-01, -2.5530e-01, -3.5023e-01],\n",
      "        [-2.6878e-01, -2.6312e-01,  2.9517e-01, -1.4540e-01, -2.7324e-01,\n",
      "          1.7972e-01,  1.2112e-02, -2.3767e-04],\n",
      "        [ 1.8994e-01, -1.7835e-02,  2.2739e-01,  1.3396e-01,  5.0134e-02,\n",
      "         -7.0727e-02,  1.7333e-01,  3.0186e-01],\n",
      "        [-9.7678e-02, -3.1403e-01, -5.9240e-02, -2.2111e-01,  1.2560e-01,\n",
      "         -8.0944e-02, -2.2800e-01, -1.8867e-01]])), ('0.block 3.2.bias', tensor([-0.2995,  0.1818, -0.0487,  0.2458])), ('1.weight', tensor([[ 0.1826,  0.2734, -0.1363, -0.0540]])), ('1.bias', tensor([0.1555]))])\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39843690-b6c4-4e63-8002-0a402bc7b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a8f95-8c2f-43f4-8b01-b2ec64247dc6",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Exercise: </font>\n",
    "#### 1. 取出block2的第二个线性层的权重值\n",
    "#### 2. 取出不属于任何一个block层的线性层的权重值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ba3544d-3dc0-4aba-9657-c00944522c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1610, -0.1948, -0.1043, -0.1215, -0.0282,  0.2583, -0.0252,  0.0601],\n",
       "        [-0.0071, -0.0540,  0.0233, -0.2840,  0.2067, -0.1861, -0.2885, -0.1971],\n",
       "        [-0.1084,  0.2378, -0.2678,  0.0487,  0.2908,  0.2647, -0.1901, -0.3354],\n",
       "        [-0.3097,  0.3002, -0.2387, -0.2053,  0.0755,  0.1713, -0.2548, -0.1546]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 取出block2的第二个线性层的权重值\n",
    "rgnet.state_dict()[\"0.block 2.2.weight\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "425004f2-c6c2-497c-816f-98d7ee2c5ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1799,  0.2817,  0.1826, -0.2483,  0.2277, -0.2317,  0.0343,  0.3402],\n",
       "        [-0.0447,  0.3260,  0.0039,  0.2002,  0.1368, -0.0641,  0.1508, -0.2149],\n",
       "        [ 0.3505,  0.1646, -0.0885, -0.0476, -0.2303,  0.0993,  0.2226, -0.1760],\n",
       "        [-0.3231,  0.1635,  0.0019,  0.1953,  0.2086,  0.2356, -0.3177,  0.1484]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f30e4fae-80fd-44f4-a09e-a2071aa5701c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1826,  0.2734, -0.1363, -0.0540]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 取出不属于任何一个block层的线性层的权重值\n",
    "rgnet.state_dict()[\"1.weight\"].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e87f0-d7a0-4df6-971a-1817c4388d8d",
   "metadata": {},
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。(**一个高级操作**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4993f94f-91a0-418c-9900-0b1762f0d422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0311,  0.0300, -0.1807,  0.4228, -0.1311, -0.3480, -0.2724,  0.4388])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a379c-2179-4b8f-b7d2-1016c718cc3e",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n",
    "\n",
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的`nn.init`模块提供了多种预置初始化方法。\n",
    "\n",
    "### **内置初始化**\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d67908a-7389-4eee-b667-89006d71c8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0109,  0.0075, -0.0115, -0.0006]), tensor(0.))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)  ### _表示inplace=True的操作\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfc754e5-67c4-4eb5-866f-951f7537e062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]\n",
    "\n",
    "### 但是，我们一般不这样去做，不把参数的初始值定为一样的值。这样，同层所有的神经元 参数 梯度全一样了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ac22519-4f2e-428a-a644-534c2066d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6972, -0.6312, -0.2773, -0.3195],\n",
      "        [ 0.3861,  0.2339, -0.4569,  0.5866],\n",
      "        [-0.2406,  0.6648, -0.6771,  0.3910],\n",
      "        [-0.6705, -0.1885, -0.1816, -0.1126],\n",
      "        [ 0.0741,  0.5355, -0.5666, -0.6383],\n",
      "        [-0.0708, -0.1077,  0.3235, -0.4137],\n",
      "        [-0.0693, -0.3128, -0.2090,  0.1416],\n",
      "        [-0.1590,  0.6419, -0.4518,  0.0842]])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "# print(net[0].weight.data[0])\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98eda8-5783-43d6-836e-50dd0af37e55",
   "metadata": {},
   "source": [
    "### **自定义初始化**\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12a02f22-a916-4a8f-a084-2dd9414ec36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n",
      "tensor([[-8.4792, -7.6218,  6.4867, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-8.4792, -7.6218,  6.4867, -0.0000],\n",
       "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [-5.8266,  6.6505, -5.1802, -5.2352],\n",
       "        [-0.0000, -9.4561, -8.8997, -6.3505],\n",
       "        [ 0.0000, -7.0935,  8.7218,  5.8544],\n",
       "        [ 0.0000,  9.1252, -6.1607, -8.6265],\n",
       "        [ 9.2162,  0.0000,  5.7082, -6.9620],\n",
       "        [ 0.0000, -0.0000,  8.2085, -0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样，我们实现了一个my_init函数来应用到net。\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        ### 这里是只保留权重大于等于5的权重值，相当于乘了一个筛选的矩阵\n",
    "\n",
    "net.apply(my_init)\n",
    "print(net[0].weight[:2])\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dea896c-de14-4363-bf4c-5f470551ed25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  9.9626,  6.1704])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 注意，我们始终可以直接设置参数。\n",
    "\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fce5ba-99d6-4e87-8d9c-27c319d1cf6a",
   "metadata": {},
   "source": [
    "## **参数绑定**\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c249069-8c86-46a7-853b-1623edab15db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n",
      "tensor([[ 1.0000e+02,  1.0000e+02,  1.0000e+02,  1.0000e+02,  1.0000e+02,\n",
      "          1.0000e+02,  1.0000e+02,  1.0000e+02],\n",
      "        [ 2.4502e-01, -1.6212e-01,  1.5172e-01,  9.9146e-02, -6.6791e-02,\n",
      "         -1.4691e-01, -3.2316e-01, -3.4960e-02],\n",
      "        [ 7.6115e-02,  1.3919e-01,  3.1959e-01, -8.4741e-02, -2.6270e-01,\n",
      "          2.3121e-01, -2.3470e-03,  3.3919e-01],\n",
      "        [ 9.5434e-02,  2.9177e-01,  3.1700e-03, -9.1332e-02, -1.8561e-01,\n",
      "         -1.8819e-01, -3.0975e-01,  3.3783e-01],\n",
      "        [-1.6530e-01,  3.2679e-02,  3.1448e-01,  2.1788e-01, -7.8854e-02,\n",
      "         -3.0098e-01, -7.5644e-02, -3.0398e-01],\n",
      "        [-2.4189e-01,  1.7559e-02,  3.9566e-02,  1.1465e-01,  2.4243e-01,\n",
      "         -2.8868e-01, -2.8383e-01, -2.8582e-01],\n",
      "        [-2.7351e-01, -4.4127e-02, -2.2347e-01,  1.8836e-01,  1.2144e-01,\n",
      "          2.0068e-01,  2.8249e-01,  1.2959e-01],\n",
      "        [ 1.8875e-01, -2.1714e-01,  7.4734e-02, -3.2013e-01,  1.2732e-01,\n",
      "         -2.5848e-01,  2.3574e-01, -6.3463e-02]])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "print(net[2].weight.data == net[4].weight.data)\n",
    "net[2].weight.data[0] = 100\n",
    "print(net[2].weight.data)\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20925e-630c-4f60-9b62-2b883298bfdc",
   "metadata": {},
   "source": [
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层\n",
    "（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efec4db-a126-4270-9a61-818e8c8daf38",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用`sec_model_construction`中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a3ed9-26d1-415a-8df7-78720b249084",
   "metadata": {},
   "source": [
    "# 自定义层\n",
    "\n",
    "深度学习成功背后的一个因素是神经网络的灵活性：\n",
    "我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。\n",
    "例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。\n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。\n",
    "在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。\n",
    "\n",
    "## 不带参数的层\n",
    "\n",
    "首先，我们(**构造一个没有任何参数的自定义层**)。\n",
    "回忆一下在 :numref:`sec_model_construction`对块的介绍，\n",
    "这应该看起来很眼熟。\n",
    "下面的`CenteredLayer`类要从其输入中减去均值。\n",
    "要构建它，我们只需继承基础层类并实现前向传播功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d97d1e25-841b-4277-8677-58d0282b6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87568500-08d8-46ea-8629-6e632dc0b924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))  # X.mean() = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd851a3-18e4-4cbb-9973-6c32c95a5b47",
   "metadata": {},
   "source": [
    "## 现在，我们可以将层作为组件合并到更复杂的模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecb28bf8-49cc-49d5-9ea1-4bd0582626c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b426d0-e799-4bab-8466-1ea057ba380d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23b15650-63e3-4469-97fd-12e3e366d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "584a6df2-c96b-4ef4-82aa-72d22418dd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4838, -0.0714, -0.2881],\n",
       "        [-0.5628, -1.5581,  0.1893],\n",
       "        [-0.1200, -1.4623, -0.4105],\n",
       "        [ 1.4874, -0.5209,  0.6994],\n",
       "        [ 0.7198, -2.4847, -0.0266]], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "add43ee9-cbad-4734-9625-8a7bfc146e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5951, 0.0000, 0.0000],\n",
       "        [0.7091, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a874fe0-1dc3-4cba-80a4-645d0384c3eb",
   "metadata": {},
   "source": [
    "我们还可以(**使用自定义层构建模型**)，就像使用内置的全连接层一样使用自定义层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e69a6cf6-5deb-49b5-a0e5-431279866033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.8393],\n",
       "        [2.0874]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d10f5-7411-4daa-8713-4a0769ae363a",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。\n",
    "* 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n",
    "* 层可以有局部参数，这些参数可以通过内置函数创建。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693c3c3-32de-4d39-9abf-ac38e942d3de",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。\n",
    "1. 设计一个返回输入数据的傅立叶系数前半部分的层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8c453-cc72-43e1-aa78-87e9a5697e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dbc7f-6382-46de-8292-12c57a5acc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25495839-2ec1-4543-a5fe-f91cc3e029ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699fb50-ee9a-4e10-8ff6-1d60208ca06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
